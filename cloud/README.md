# QuantGod Cloud Infrastructure ‚òÅÔ∏è

Este diret√≥rio cont√©m o pipeline de ETL (Extract, Transform, Load) projetado para processar terabytes de dados de Orderbook (L2) na nuvem (RunPod) de forma eficiente, utilizando streaming de dados e otimiza√ß√£o de mem√≥ria.

---

## üìÇ Estrutura de Arquivos e Pastas

### 1. `configs/` (Configura√ß√µes)
Arquivos YAML que definem o comportamento do pipeline.
*   **`cloud_config.yaml`**: Configura√ß√£o oficial para produ√ß√£o no RunPod. Aponta para o diret√≥rio de dados montado via `rclone`.
*   **`test_local.yaml`**: Configura√ß√£o para testes em ambiente de desenvolvimento. Aponta para pastas locais (`data/L2/raw/l2_samples`).

**Par√¢metros Principais:**
*   `paths.rclone_mount`: Caminho do mount do Google Drive.
*   `paths.processed_output`: Destino dos arquivos `.parquet`.
*   `etl.orderbook_levels`: N√≠vel do **Hard Cut** (Ex: 200).
*   `features.apply_zscore`: Ativa/Desativa a normaliza√ß√£o estat√≠stica.

### 2. `etl/` (M√≥dulos de Processamento)
O motor do processamento, dividido em responsabilidades modulares:

*   **`extract.py`**: Implementa a l√≥gica **Zero-Copy**. Ele abre os ZIPs diretamente do mount e l√™ o conte√∫do (JSON/CSV) linha por linha em buffer de mem√≥ria, sem nunca descompactar arquivos no disco f√≠sico do RunPod.
*   **`transform.py`**: O c√©rebro do pipeline.
    *   Reconstr√≥i o Orderbook a partir de snapshots e deltas.
    *   Aplica o **Hard Cut 200** (mant√©m estritamente os top 200 n√≠veis).
    *   Realiza amostragem temporal (1s ticks) e resampling (1min OHLCV).
    *   Calcula Micro-Price, Spread e IOBI.
    *   Aplica **Stationarity Fix** (Log-Returns para pre√ßos e Log1p para volume).
*   **`load.py`**: Gerencia a persist√™ncia. Utiliza o formato **Apache Parquet** com compress√£o **Snappy** para garantir leitura ultra-r√°pida durante o treino do modelo.
*   **`validate.py`**: Garante a qualidade do dado. Verifica se h√° NaNs, valores infinitos, se a ordem cronol√≥gica est√° correta e se existem "gaps" de tempo excessivos.

### 3. `orchestration/` (Coordena√ß√£o)
*   **`run_pipeline.py`**: O ponto de entrada. Ele coordena o fluxo entre todos os m√≥dulos acima. Suporta a passagem de arquivos de config via terminal:
    `python -m cloud.orchestration.run_pipeline cloud/configs/test_local.yaml`

### 4. `setup_cloud.sh` (Automa√ß√£o de Ambiente)
Script bash para preparar a inst√¢ncia Linux (RunPod).
*   Instala pacotes do sistema (`rclone`, `python3-pip`).
*   Cria o ambiente virtual `.venv`.
*   Instala as depend√™ncias de Python.
*   Cria a √°rvore de diret√≥rios oficial (`data/L2/pre_processed`, `data/artifacts`, etc.).

---

## üöÄ Como Usar

### Passo 1: Preparar a m√°quina
```bash
cd cloud
chmod +x setup_cloud.sh
./setup_cloud.sh
```

### Passo 2: Configurar o Rclone
Voc√™ precisa configurar sua conex√£o com o Google Drive:
```bash
rclone config
```

### Passo 3: Rodar o Processamento
Ative o ambiente e execute o pipeline:
```bash
source .venv/bin/activate
# Para produ√ß√£o (RunPod):
python -m cloud.orchestration.run_pipeline
# Para testes (Local):
python -m cloud.orchestration.run_pipeline cloud/configs/test_local.yaml
```

---

## üõ†Ô∏è Requisitos T√©cnicos (`requirements.txt`)
O pipeline depende de:
*   `polars` / `pandas`: Processamento de dados de alta performance.
*   `pyarrow`: Engine para escrita de Parquet.
*   `scikit-learn`: Para aplica√ß√£o do `StandardScaler` (Z-Score).
*   `tqdm`: Barras de progresso para monitoramento de grandes volumes.
*   `pyyaml`: Leitura dos arquivos de configura√ß√£o.
