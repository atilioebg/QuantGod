# ğŸ“˜ SAIMP: Data Collection Module

> **Status**: Em ExecuÃ§Ã£o (Coleta de Dados Ativa) ğŸŸ¢  
> **VersÃ£o**: 1.0 (MVP)

## 1. VisÃ£o Geral
Este documento serve como manual tÃ©cnico e operacional para o mÃ³dulo de ingestÃ£o de dados do **SAIMP**. O sistema foi projetado para capturar e armazenar dados de alta frequÃªncia do mercado de criptomoedas (Binance Futures - BTCUSDT) com foco em **baixa latÃªncia** e **eficiÃªncia de armazenamento**.

### Filosofia "GeoAI"
Tratamos o mercado nÃ£o apenas como uma sÃ©rie temporal, mas como um terreno topogrÃ¡fico em evoluÃ§Ã£o:
- **Order Book (Liq. Passiva)**: Paredes, suportes e resistÃªncias (Montanhas/Vales).
- **ExecuÃ§Ãµes (Liq. Agressiva)**: Fluxo de ordens que consome a liquidez (ErosÃ£o).

## 2. Arquitetura de Dados

### Stack TecnolÃ³gica
- **Data Engine**: `Polars` (Rust-backed, High Performance).
- **Storage**: `Apache Parquet` (CompressÃ£o `zstd`, Colunar).
- **Async I/O**: `asyncio`, `aiohttp`, `websockets`.
- **Config**: `pydantic-settings`.

### Ãrvore de DiretÃ³rios de Dados
```bash
data/
â””â”€â”€ raw/
    â”œâ”€â”€ historical/           # Dados HistÃ³ricos (Backtesting)
    â”‚   â”œâ”€â”€ klines_YYYY-MM.parquet
    â”‚   â””â”€â”€ aggTrades_YYYY-MM.parquet
    â””â”€â”€ stream/               # Dados em Tempo Real (Live Trading/Training)
        â”œâ”€â”€ depth/YYYY-MM-DD/ # Snapshots do Order Book
        â””â”€â”€ trades/YYYY-MM-DD/# ExecuÃ§Ãµes em Tempo Real
```

## 3. Manual de OperaÃ§Ã£o

Para manter a coleta ativa, dois processos distintos devem rodar em paralelo.

### ğŸ”´ Terminal 1: O Gravador (Stream)
**FunÃ§Ã£o**: Capturar o "agora". Conecta ao WebSocket da Binance e grava o Order Book e Trades.
* **FrequÃªncia de Flush**: A cada 15 minutos ou 50MB de buffer.
* **ResiliÃªncia**: ReconexÃ£o automÃ¡tica com backoff exponencial.

```powershell
# Executar no Terminal 1
python -m src.collectors.stream
```

### ğŸšœ Terminal 2: A Escavadeira (Historical)
**FunÃ§Ã£o**: Capturar o "passado". Baixa dados histÃ³ricos mensais da Binance Vision.
* **Performance**: Download paralelo (3 meses p/ vez) e processamento em memÃ³ria (sem unzip em disco).
* **Dados**: `aggTrades` (Tick-by-tick) e `klines` (1m).

```powershell
# Executar no Terminal 2
python -m src.collectors.historical
```

## 4. DicionÃ¡rio de Dados

### A. HistÃ³rico (`data/raw/historical/`)
Dados oficiais da Binance Vision, consolidados mensalmente.

| Tipo | Nome do Arquivo | ConteÃºdo Principal | Uso |
|:---|:---|:---|:---|
| **Klines** | `klines_YYYY-MM.parquet` | OHLCV (1m), Volume, Taker Buy Vol | Contexto Macro, TendÃªncia |
| **Trades** | `aggTrades_YYYY-MM.parquet` | PreÃ§o, Qtd, Tempo, IsBuyerMaker | AnÃ¡lise de Fluxo, OFI, Delta |

### B. Streaming (`data/raw/stream/`)
Dados proprietÃ¡rios gravados em tempo real. Essenciais para treinar a IA a "ler a fita".

| Tipo | ConteÃºdo | Estrutura | Uso |
|:---|:---|:---|:---|
| **Depth** | Order Book (Top 20 levels) | **bids**: `[[price, qty], ...]`<br>**asks**: `[[price, qty], ...]` | Identificar Liquidez, Spoofing |
| **Trade** | ExecuÃ§Ãµes em Tempo Real | Igual ao aggTrades histÃ³rico | Sincronizar erosÃ£o da liquidez |

## 5. Auditoria de Dados
Para garantir que o download foi completado sem buracos, execute o script de auditoria:

```powershell
python src/audit/check_completeness.py
```

Para verificar se o Stream estÃ¡ capturando dados corretamente:
```powershell
python src/audit/check_stream.py
```
*SaÃ­da esperada*: Detalhes do Ãºltimo arquivo `.parquet` gerado (Tamanho, Colunas, Amostra).

## 6. Protocolo de RecuperaÃ§Ã£o (Disaster Recovery)

### O que fazer se o PC desligar ou a internet cair?
1. **NÃ£o entre em pÃ¢nico**. O dado atÃ© o Ãºltimo flush (15m atrÃ¡s) estÃ¡ salvo.
2. **Reinicie imediatamente** o script `stream.py`.
3. **Mapeie o Gap**: O perÃ­odo offline serÃ¡ um "buraco" nos dados de *depth*.
    - *Impacto*: A IA perderÃ¡ o contexto de curto prazo.
    - *SoluÃ§Ã£o futura*: O pipeline de treino ignorarÃ¡ janelas com gaps > 15min.

### Backfill (Preenchimento)
Se o `stream.py` ficar dias desligado, vocÃª pode baixar os dias perdidos usando o `historical.py` (quando a Binance disponibilizar os dados mensais/diÃ¡rios), mas perderÃ¡ a granularidade fina do Order Book (Depth) desse perÃ­odo.

## 7. Roadmap & PrÃ³ximos Passos ğŸš€

- [x] **Fase 1: Coleta de Dados** (Infraestrutura Pronta)
- [ ] **Fase 2: Processamento (ETL)** 
    - Unificar Stream + HistÃ³rico.
    - Calcular Features (OFI, VPIN, Microstructure noise).
    - Gerar Tensores (Imagens espectrais do Book).
- [ ] **Fase 3: Treinamento** (ViViT Model)
- [ ] **Fase 4: ProduÃ§Ã£o** (Live Inference)
