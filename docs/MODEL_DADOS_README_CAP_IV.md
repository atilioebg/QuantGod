# ğŸ¤– SAIMP: The AI Brain Module (CAPÃTULO IV)

> **Status**: ConcluÃ­do (Fase 4) âœ…  
> **VersÃ£o**: 4.0 (ViViT: Video Vision Transformer)
> **DependÃªncia**: Requer `LABELLING_DADOS_README_CAP_III.md` (Targets)

## 1. VisÃ£o Geral
Este documento detalha o "CÃ³rtex" do **SAIMP**. Aqui, os Tensores 4D (Input) encontram os RÃ³tulos de Risco (Target) dentro de uma arquitetura neural hÃ­brida de Ãºltima geraÃ§Ã£o.

O objetivo nÃ£o Ã© apenas processar dados, mas **entender a narrativa do mercado**. Para isso, combinamos duas superpotÃªncias da IA:
1.  **VisÃ£o Computacional (CNN)**: Para "ver" a estrutura do Order Book a cada instante.
2.  **Processamento Sequencial (Transformers)**: Para "lembrar" a evoluÃ§Ã£o do fluxo ao longo do tempo.

---

## 2. A Arquitetura: SAIMPViViT

O modelo `SAIMPViViT` (`src/models/vivit.py`) Ã© uma adaptaÃ§Ã£o de arquiteturas de classificaÃ§Ã£o de vÃ­deo para o mercado financeiro.

### A. O Olho: Spatial Feature Extractor (CNN 1D)
Antes de entender o tempo, precisamos entender o espaÃ§o (PreÃ§o e Volume).
*   **Input**: Um Ãºnico snapshot do tensor `(4 canais, 128 nÃ­veis)`.
*   **Camadas**:
    *   3 Blocos de ConvoluÃ§Ã£o 1D (`Conv1d -> BatchNorm -> ReLU -> MaxPool`).
    *   Reduz a dimensÃ£o de altura (128 nÃ­veis) para um vetor latente denso (`d_model=128`).
*   **FunÃ§Ã£o**: Aprende padrÃµes visuais como "Paredes de Compra", "AbsorÃ§Ã£o", "Spread Vazio".

### B. A MemÃ³ria: Temporal Transformer Encoder
Depois de extrair as caracterÃ­sticas de cada frame, precisamos conectar os pontos.
*   **Input**: Uma sequÃªncia de vetores latentes `(Tempo=96, Features=128)`.
*   **Positional Encoding**: Adiciona informaÃ§Ã£o de ordem temporal (quem veio antes de quem).
*   **Encoder Layer**: Mecanismo de **Self-Attention** que permite Ã  rede relacionar um evento no inÃ­cio da janela (ex: agressÃ£o forte) com o resultado final (ex: rompimento).
*   **Output**: O estado oculto do Ãºltimo timestep, contendo o resumo de toda a sequÃªncia.

### C. A DecisÃ£o: Classification Head
*   **Camadas**: MLP (Linear -> ReLU -> Dropout -> Linear).
*   **Output**: Logits para 3 classes (`Neutral`, `Sell`, `Buy`).

---

## 3. O Processo de Treinamento (`train.py`)

O script `src/training/train.py` Ã© a academia onde o modelo exercita seus neurÃ´nios.

### Pipeline de Dados (On-the-Fly ETL)
Os datasets de **Treino** e **ValidaÃ§Ã£o** sÃ£o criados **dinamicamente (On-the-Fly)** durante o treinamento, e **nÃ£o antes**.

Isso Ã© feito atravÃ©s da classe `StreamingDataset` que implementamos. Aqui estÃ¡ o fluxo exato do que acontece no cÃ³digo:

#### 1. DefiniÃ§Ã£o Logica (Metadados)
No inÃ­cio do `train.py`, apenas definimos **quais dias** pertencem a cada conjunto. Nada Ã© carregado na memÃ³ria neste momento.
```python
# train.py
train_dataset = StreamingDataset(TRAIN_MONTHS)
train_dataset.set_date_range("2026-01-01", "2026-01-21") # Define o intervalo de data

val_dataset = StreamingDataset(VAL_MONTHS)
val_dataset.set_date_range("2026-01-22", "2026-01-31")   # Define o intervalo de data
```

#### 2. GeraÃ§Ã£o Just-in-Time (Durante o Loop)
Quando o `DataLoader` pede dados (no loop `for batch_idx, (data, target) in enumerate(train_loader)`), o `StreamingDataset` entra em aÃ§Ã£o:

1.  **Carrega um Chunk (Dia)**: LÃª o arquivo Parquet original (`aggTrades` e `klines`) apenas para o dia atual.
2.  **Processa em MemÃ³ria**:
    *   Roda `build_simulated_book` (recria o Order Book).
    *   Roda `generate_labels` (cria os alvos Buy/Sell/Hold).
    *   Faz o Join dos dois.
3.  **Cria Tensores**: Converte os dados processados para tensores PyTorch (`build_tensor_4d`).
4.  **Entrega (Yield)**: Entrega as sequÃªncias uma a uma para o `DataLoader`.
5.  **Descarta**: Assim que o dia termina, ele **apaga** tudo da memÃ³ria (`gc.collect`) e carrega o prÃ³ximo dia.

#### Por que fizemos assim?
*   **Vantagem**: **Economia Extrema de RAM**. VocÃª consegue treinar com terabytes de dados usando apenas ~2GB de RAM, pois sÃ³ carrega um dia por vez.
*   **Desvantagem**: **Uso Intenso de CPU**. A CPU precisa processar (simular book, gerar labels) enquanto a GPU treina.

Se criÃ¡ssemos os datasets **antes** (salvando em disco como tensores prontos), o treino seria mais rÃ¡pido (menos CPU), mas ocuparia muito espaÃ§o em disco e exigiria um prÃ©-processamento longo. A abordagem atual prioriza a capacidade de rodar em hardware modesto.

### HiperparÃ¢metros (ConfiguraÃ§Ã£o PadrÃ£o)
| ParÃ¢metro | Valor | DescriÃ§Ã£o |
|:---|:---|:---|
| `seq_len` | 96 | Janela de observaÃ§Ã£o (ex: 96 snapshots de 15m = 24h). |
| `input_channels` | 4 | Bids, Asks, OFI, Activity. |
| `price_levels` | 128 | Altura da imagem do book. |
| `d_model` | 128 | Tamanho do vetor latente (embedding). |
| `batch_size` | 32 | Amostras por passo de treino. |
| `learning_rate` | 1e-4 | Taxa de aprendizado (AdamW). |

### FunÃ§Ã£o de Perda (Loss Function)
Usamos **CrossEntropyLoss** com **Class Weights**.
*   **Problema**: O mercado fica "Neutro" (Class 0) na maior parte do tempo.
*   **SoluÃ§Ã£o**: Penalizamos mais o erro nas classes raras (Compra/Venda).
    *   Peso Neutro: 1.0
    *   Peso Buy/Sell: 2.0

---

## 4. ValidaÃ§Ã£o e MÃ©tricas

Como saber se a IA nÃ£o estÃ¡ apenas "decorando" o passado?

### Split Temporal (Walk-Forward)
Jamais misturamos o futuro com o passado.
*   **Treino**: Jan-Set (Dados Antigos).
*   **ValidaÃ§Ã£o**: Out-Dez (Dados Recentes).

### MÃ©tricas Chave
1.  **Loss (Perda)**: Deve diminuir consistentemente no treino e validaÃ§Ã£o. Se subir na validaÃ§Ã£o, Ã© *Overfitting*.
2.  **AcurÃ¡cia**: % de acertos totais. (Cuidado: num mercado lateral, chutar "Neutro" dÃ¡ alta acurÃ¡cia mas lucro zero).
3.  **Precision/Recall (Futuro)**: Focaremos em precisÃ£o de entradas (evitar falsos positivos).

---

## 5. Como Treinar

```powershell
# 1. Certifique-se de ter dados histÃ³ricos em data/raw/historical
# 2. Execute o script de treino
python -m src.training.train
```

**O que acontece:**
1.  O script verifica se hÃ¡ GPU (`cuda`) disponÃ­vel.
2.  Carrega os dados e inicia o loop de Ã©pocas.
3.  Imprime `Loss` e `Acc` a cada Ã©poca.
4.  Salva o melhor modelo em `data/SAIMP_v1.pth`.

---

## 6. PrÃ³ximos Passos (Fase 5 - ProduÃ§Ã£o) ğŸš€

O cÃ©rebro estÃ¡ criado. Agora precisamos colocÃ¡-lo no corpo do robÃ´.

- [ ] **Inference Engine**: Script que carrega o `.pth` e roda previsÃµes em tempo real conectado ao `stream.py`.
- [ ] **Risk Manager**: MÃ³dulo que decide o tamanho da posiÃ§Ã£o baseado na confianÃ§a da IA.
- [ ] **Execution Algo**: O "dedo no gatilho" que envia ordens via API da Binance.

---
> *SAIMP - InteligÃªncia Artificial aplicada Ã  Microestrutura de Mercado.*
